{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_model_2.weight.l1</th>\n",
       "      <th>eval_model_2.weight.l2</th>\n",
       "      <th>eval_model_2.bias.l1</th>\n",
       "      <th>eval_model_2.bias.l2</th>\n",
       "      <th>eval_model_5.weight.l1</th>\n",
       "      <th>eval_model_5.weight.l2</th>\n",
       "      <th>eval_model_5.bias.l1</th>\n",
       "      <th>eval_model_5.bias.l2</th>\n",
       "      <th>eval_model_8.weight.l1</th>\n",
       "      <th>...</th>\n",
       "      <th>eval_model_8.bias.l2</th>\n",
       "      <th>eval_model_11.weight.l1</th>\n",
       "      <th>eval_model_11.weight.l2</th>\n",
       "      <th>eval_model_11.bias.l1</th>\n",
       "      <th>eval_model_11.bias.l2</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_recall1</th>\n",
       "      <th>tag</th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.183503</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.010206</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.058167</td>\n",
       "      <td>0.045780</td>\n",
       "      <td>0.054516</td>\n",
       "      <td>0.050342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.054326</td>\n",
       "      <td>0.062981</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.048558</td>\n",
       "      <td>1.993463</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.881180</td>\n",
       "      <td>0.009166</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.051007</td>\n",
       "      <td>0.058983</td>\n",
       "      <td>0.046819</td>\n",
       "      <td>0.055455</td>\n",
       "      <td>0.050968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060825</td>\n",
       "      <td>0.060008</td>\n",
       "      <td>0.070406</td>\n",
       "      <td>0.041463</td>\n",
       "      <td>0.050881</td>\n",
       "      <td>1.777954</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.727290</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.012271</td>\n",
       "      <td>0.051520</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.047424</td>\n",
       "      <td>0.056055</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061617</td>\n",
       "      <td>0.064234</td>\n",
       "      <td>0.076131</td>\n",
       "      <td>0.038896</td>\n",
       "      <td>0.052805</td>\n",
       "      <td>1.667873</td>\n",
       "      <td>0.4034</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.644757</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.010924</td>\n",
       "      <td>0.010887</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>0.060226</td>\n",
       "      <td>0.047938</td>\n",
       "      <td>0.056602</td>\n",
       "      <td>0.051939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062509</td>\n",
       "      <td>0.067477</td>\n",
       "      <td>0.080575</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.055904</td>\n",
       "      <td>1.607535</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.583154</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>0.014785</td>\n",
       "      <td>0.052323</td>\n",
       "      <td>0.060726</td>\n",
       "      <td>0.048422</td>\n",
       "      <td>0.057218</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063575</td>\n",
       "      <td>0.070224</td>\n",
       "      <td>0.084386</td>\n",
       "      <td>0.039928</td>\n",
       "      <td>0.060964</td>\n",
       "      <td>1.557674</td>\n",
       "      <td>0.4511</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.873335</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>0.017785</td>\n",
       "      <td>0.070681</td>\n",
       "      <td>0.093177</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>0.071842</td>\n",
       "      <td>0.098068</td>\n",
       "      <td>0.134804</td>\n",
       "      <td>0.059707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124232</td>\n",
       "      <td>0.125564</td>\n",
       "      <td>0.152852</td>\n",
       "      <td>0.247697</td>\n",
       "      <td>0.299232</td>\n",
       "      <td>1.546749</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>var</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.859384</td>\n",
       "      <td>0.014309</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.071791</td>\n",
       "      <td>0.094662</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.072118</td>\n",
       "      <td>0.099092</td>\n",
       "      <td>0.136257</td>\n",
       "      <td>0.059895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125157</td>\n",
       "      <td>0.126689</td>\n",
       "      <td>0.154208</td>\n",
       "      <td>0.248507</td>\n",
       "      <td>0.300656</td>\n",
       "      <td>1.552890</td>\n",
       "      <td>0.5199</td>\n",
       "      <td>var</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.852763</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.018180</td>\n",
       "      <td>0.072807</td>\n",
       "      <td>0.096082</td>\n",
       "      <td>0.059333</td>\n",
       "      <td>0.072377</td>\n",
       "      <td>0.100128</td>\n",
       "      <td>0.137638</td>\n",
       "      <td>0.060069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125970</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.155458</td>\n",
       "      <td>0.248955</td>\n",
       "      <td>0.301777</td>\n",
       "      <td>1.557548</td>\n",
       "      <td>0.5165</td>\n",
       "      <td>var</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.845348</td>\n",
       "      <td>0.014613</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.073821</td>\n",
       "      <td>0.097403</td>\n",
       "      <td>0.059490</td>\n",
       "      <td>0.072612</td>\n",
       "      <td>0.101071</td>\n",
       "      <td>0.138930</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126710</td>\n",
       "      <td>0.128618</td>\n",
       "      <td>0.156576</td>\n",
       "      <td>0.249448</td>\n",
       "      <td>0.302861</td>\n",
       "      <td>1.549425</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>var</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.836433</td>\n",
       "      <td>0.014768</td>\n",
       "      <td>0.018577</td>\n",
       "      <td>0.074792</td>\n",
       "      <td>0.098758</td>\n",
       "      <td>0.059652</td>\n",
       "      <td>0.072863</td>\n",
       "      <td>0.101952</td>\n",
       "      <td>0.140171</td>\n",
       "      <td>0.060388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127361</td>\n",
       "      <td>0.129536</td>\n",
       "      <td>0.157749</td>\n",
       "      <td>0.249728</td>\n",
       "      <td>0.303662</td>\n",
       "      <td>1.547398</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>var</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_loss  eval_model_2.weight.l1  eval_model_2.weight.l2  \\\n",
       "0      2.183503                0.009075                0.010514   \n",
       "1      1.881180                0.009166                0.010669   \n",
       "2      1.727290                0.009247                0.010805   \n",
       "3      1.644757                0.009317                0.010924   \n",
       "4      1.583154                0.009386                0.011038   \n",
       "..          ...                     ...                     ...   \n",
       "245    0.873335                0.014157                0.017785   \n",
       "246    0.859384                0.014309                0.017981   \n",
       "247    0.852763                0.014461                0.018180   \n",
       "248    0.845348                0.014613                0.018377   \n",
       "249    0.836433                0.014768                0.018577   \n",
       "\n",
       "     eval_model_2.bias.l1  eval_model_2.bias.l2  eval_model_5.weight.l1  \\\n",
       "0                0.008344              0.010206                0.050375   \n",
       "1                0.009067              0.011281                0.051007   \n",
       "2                0.009921              0.012271                0.051520   \n",
       "3                0.010887              0.013409                0.051942   \n",
       "4                0.011912              0.014785                0.052323   \n",
       "..                    ...                   ...                     ...   \n",
       "245              0.070681              0.093177                0.058980   \n",
       "246              0.071791              0.094662                0.059160   \n",
       "247              0.072807              0.096082                0.059333   \n",
       "248              0.073821              0.097403                0.059490   \n",
       "249              0.074792              0.098758                0.059652   \n",
       "\n",
       "     eval_model_5.weight.l2  eval_model_5.bias.l1  eval_model_5.bias.l2  \\\n",
       "0                  0.058167              0.045780              0.054516   \n",
       "1                  0.058983              0.046819              0.055455   \n",
       "2                  0.059667              0.047424              0.056055   \n",
       "3                  0.060226              0.047938              0.056602   \n",
       "4                  0.060726              0.048422              0.057218   \n",
       "..                      ...                   ...                   ...   \n",
       "245                0.071842              0.098068              0.134804   \n",
       "246                0.072118              0.099092              0.136257   \n",
       "247                0.072377              0.100128              0.137638   \n",
       "248                0.072612              0.101071              0.138930   \n",
       "249                0.072863              0.101952              0.140171   \n",
       "\n",
       "     eval_model_8.weight.l1  ...  eval_model_8.bias.l2  \\\n",
       "0                  0.050342  ...              0.059800   \n",
       "1                  0.050968  ...              0.060825   \n",
       "2                  0.051497  ...              0.061617   \n",
       "3                  0.051939  ...              0.062509   \n",
       "4                  0.052330  ...              0.063575   \n",
       "..                      ...  ...                   ...   \n",
       "245                0.059707  ...              0.124232   \n",
       "246                0.059895  ...              0.125157   \n",
       "247                0.060069  ...              0.125970   \n",
       "248                0.060223  ...              0.126710   \n",
       "249                0.060388  ...              0.127361   \n",
       "\n",
       "     eval_model_11.weight.l1  eval_model_11.weight.l2  eval_model_11.bias.l1  \\\n",
       "0                   0.054326                 0.062981               0.040626   \n",
       "1                   0.060008                 0.070406               0.041463   \n",
       "2                   0.064234                 0.076131               0.038896   \n",
       "3                   0.067477                 0.080575               0.037249   \n",
       "4                   0.070224                 0.084386               0.039928   \n",
       "..                       ...                      ...                    ...   \n",
       "245                 0.125564                 0.152852               0.247697   \n",
       "246                 0.126689                 0.154208               0.248507   \n",
       "247                 0.127726                 0.155458               0.248955   \n",
       "248                 0.128618                 0.156576               0.249448   \n",
       "249                 0.129536                 0.157749               0.249728   \n",
       "\n",
       "     eval_model_11.bias.l2  eval_loss  eval_recall1   tag  run epoch  \n",
       "0                 0.048558   1.993463        0.2844  none    1     1  \n",
       "1                 0.050881   1.777954        0.3639  none    1     2  \n",
       "2                 0.052805   1.667873        0.4034  none    1     3  \n",
       "3                 0.055904   1.607535        0.4286  none    1     4  \n",
       "4                 0.060964   1.557674        0.4511  none    1     5  \n",
       "..                     ...        ...           ...   ...  ...   ...  \n",
       "245               0.299232   1.546749        0.5175   var    1    46  \n",
       "246               0.300656   1.552890        0.5199   var    1    47  \n",
       "247               0.301777   1.557548        0.5165   var    1    48  \n",
       "248               0.302861   1.549425        0.5176   var    1    49  \n",
       "249               0.303662   1.547398        0.5216   var    1    50  \n",
       "\n",
       "[250 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''setup = {\n",
    "    \"db\": [\"cifar-10\"], #[\"mnist\", \"cifar-10\", \"cifar-100\", \"svhn\"]\n",
    "    \"dim\": [\"d200\"], #[\"d100\", \"d200\", \"d500\"]\n",
    "    \"do\": [\"norm\", \"reg\"],\n",
    "    \"on\": [\"x\", \"w\"],\n",
    "    \"std\": [\"s1o4\", \"s1o2\", \"s1o1\"],\n",
    "    \"lyr\": [\"no\", \"l1\",  \"l2\", \"l3\", \"all\"],\n",
    "    \"run\": [\"1\"],\n",
    "}'''\n",
    "\n",
    "setup = {\n",
    "    \"tag\": [\"none\", \"base\", \"mean\", \"cov\", \"var\"],\n",
    "    \"run\": [\"1\"],\n",
    "}\n",
    "\n",
    "def read_metrics(setup):\n",
    "    dfs = []\n",
    "    for values in list(itertools.product(*setup.values())):\n",
    "        label = \"_\".join(values)\n",
    "        title = dict(zip(setup.keys(), values))\n",
    "        try:\n",
    "            df = pd.read_json(f\"outputs/{label}/logs.json\")\n",
    "            df = df.assign(**title)\n",
    "            df[\"epoch\"] = df.index + 1\n",
    "            dfs.append(df)\n",
    "        except:\n",
    "            pass\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def read_checkpoints(setup):\n",
    "    data = []\n",
    "    for values in list(itertools.product(*setup.values())):\n",
    "        label = \"_\".join(values)\n",
    "        title = dict(zip(setup.keys(), values))\n",
    "        try:\n",
    "            state = torch.load(f\"outputs/{label}/checkpoint.pt\")\n",
    "        except:\n",
    "            continue\n",
    "        data.append({\n",
    "            **title,\n",
    "            **{f\"w{i}\": w.norm().item() for i, w in enumerate(state[\"ml\"][\"w\"])},\n",
    "            **{f\"b{i}\": b.norm().item() for i, b in enumerate(state[\"ml\"][\"b\"])},\n",
    "            **{f\"x{i}\": m.diagonal().sum().sqrt().item() for i, m in enumerate(state[\"ml\"][\"m2\"])},\n",
    "        })\n",
    "\n",
    "read_checkpoints(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = [\"cifar-10\"] #[\"mnist\", \"cifar-10\", \"cifar-100\", \"svhn\"]\n",
    "dims = [\"d200\"] #[\"d100\", \"d200\", \"d500\"]\n",
    "dos = [\"norm\", \"reg\"]\n",
    "ons = [\"x\", \"w\"]\n",
    "stds = [\"s1o4\", \"s1o2\", \"s1o1\"]\n",
    "lyrs = [\"no\", \"l1\",  \"l2\", \"l3\", \"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for db, dim, do, on, std, lyr in itertools.product(dbs, dims, dos, ons, stds, lyrs):\n",
    "  label = f\"{db}_{dim}_{do}_{on}_{std}_{lyr}_1\"\n",
    "  try:\n",
    "    state = torch.load(f\"outputs/{label}/analysis.pt\")\n",
    "  except:\n",
    "    continue\n",
    "  data.append({\n",
    "    \"db\": db, \"dim\": dim, \"do\": do, \"on\": on, \"std\": std, \"lyr\": lyr,\n",
    "    **{f\"w{i}\": w.norm().item() for i, w in enumerate(state[\"ml\"][\"w\"])},\n",
    "    **{f\"b{i}\": b.norm().item() for i, b in enumerate(state[\"ml\"][\"b\"])},\n",
    "    **{f\"x{i}\": m.diagonal().sum().sqrt().item() for i, m in enumerate(state[\"ml\"][\"m2\"])},\n",
    "  })\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "mets = {\"state\": \"x\", \"weight\": \"w\", \"bias\": \"b\"}\n",
    "for i, (db, std) in enumerate(itertools.product(dbs, stds)):\n",
    "  ncols = len(dos)*len(ons)\n",
    "  nrows = len(mets)\n",
    "  _, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, sharex=\"col\", sharey=\"row\",\n",
    "    figsize=(ncols*2.5, nrows*2), dpi=100, facecolor=\"w\")\n",
    "  for j, (on, do) in enumerate(itertools.product(ons, dos)):\n",
    "    sdf = df[(df[\"db\"]==db)&(df[\"dim\"]==dim)&(df[\"do\"]==do)&(df[\"on\"]==on)&(df[\"std\"]==std)]\n",
    "    if len(sdf) == 0:\n",
    "      continue\n",
    "    sdf = sdf.set_index(\"lyr\").drop(columns=[\"db\", \"dim\", \"do\", \"on\", \"std\"])\n",
    "    sdf = sdf.divide(sdf.loc[\"no\"]).drop(\"no\")\n",
    "    for k, (met, prefix) in enumerate(mets.items()):\n",
    "      cols = [col for col in sdf.columns if col.startswith(prefix)]\n",
    "      sdf[cols].plot.bar(ax=axes[k][j], legend=False, rot=0)\n",
    "      axes[k][j].set_xlabel(f\"{do} {on} {std}\")\n",
    "      axes[k][j].set_ylabel(met)\n",
    "      axes[k][j].set_ylim((0, 1.5))\n",
    "      axes[k][j].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(config):\n",
    "  data = []\n",
    "  for values in itertools.product(*config.values()):\n",
    "    label = \"_\".join(values)+\"_1\"\n",
    "    title = {k: v for k, v in zip(config.keys(), values)}\n",
    "    if title[\"detach\"] == \"ans\":\n",
    "      db, dim, do, on, detach, std, lyr = values\n",
    "      label = f\"{db}_{dim}_norm_{on}_{std}_{lyr}_1\"\n",
    "    try:\n",
    "      with open(f\"outputs/{label}/logs.json\") as f:\n",
    "          logs = json.load(f)\n",
    "      for i, log in enumerate(logs):\n",
    "          data.append({**title, \"epoch\": i+1, **log})\n",
    "    except:\n",
    "      pass\n",
    "  return pd.DataFrame(data)\n",
    "\n",
    "def plot_curve(config, x, y, g):\n",
    "  df = read(config)\n",
    "  values = list(itertools.product(*[config[k] for k in x]))\n",
    "  fields = y\n",
    "\n",
    "  ncols = len(values)\n",
    "  nrows = len(fields)\n",
    "  _, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, sharex=True, sharey=\"row\",\n",
    "    figsize=(ncols*2.5, nrows*2), dpi=100, facecolor=\"w\")\n",
    "\n",
    "  for i, vs in enumerate(values):\n",
    "    label = \" \".join(vs)\n",
    "    sdf = df\n",
    "    for k, v in zip(x, vs):\n",
    "      sdf = sdf[sdf[k] == v]\n",
    "    if len(sdf) == 0:\n",
    "      continue\n",
    "    sdf = sdf.set_index(\"epoch\").groupby(g)\n",
    "    for j, field in enumerate(fields):\n",
    "      ax = axes[j][i]\n",
    "      sdf[field].plot(ax=ax, legend=False)\n",
    "      ax.set_xlabel(label)\n",
    "      ax.set_ylabel(field)\n",
    "      ax.grid()\n",
    "      if j == 0:\n",
    "        ax.legend()\n",
    "      if field == \"eval_top1\":\n",
    "        ax.set_ylim((0.45, 0.6))\n",
    "\n",
    "config = {\n",
    "  \"db\": [\"cifar-10\"],\n",
    "  \"dim\": [\"d200\"],\n",
    "  \"do\": [\"reg\"],\n",
    "  \"on\": [\"x\", \"w\"],\n",
    "  \"detach\": [\"ans\", \"no\"],\n",
    "  \"std\": [\"s1o1\"],\n",
    "  \"lyr\": [\"l1\", \"l2\", \"l3\"],\n",
    "}\n",
    "\n",
    "metsets = [\n",
    "  [\"train_loss\", \"eval_loss\", \"eval_top1\"], \n",
    "  [f\"train_model_{i}.state.l1\" for i in (4, 6, 8)] + [\"train_model.output.l1\"],\n",
    "  [f\"eval_model_{i}.weight.l1\" for i in (1, 3, 5, 7)],\n",
    "]\n",
    "for mets in metsets:\n",
    "  plot_curve(config, x=[\"do\", \"std\", \"on\", \"std\", \"lyr\"], y=mets, g=\"detach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for db, dim, do, on, std, lyr in itertools.product(dbs, dims, dos, ons, stds, lyrs):\n",
    "  label = f\"{db}_{dim}_{do}_{on}_{std}_{lyr}_1\"\n",
    "  title = {\"db\": db, \"dim\": dim, \"do\": do, \"std\": std, \"lyr\": lyr, \"on\": on}\n",
    "  try:\n",
    "    with open(f\"outputs/{label}/logs.json\") as f:\n",
    "        logs = json.load(f)\n",
    "    for i, log in enumerate(logs):\n",
    "        data.append({**title, \"epoch\": i+1, **log})\n",
    "  except:\n",
    "    pass\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "metsets = [\n",
    "  [\"train_loss\", \"eval_loss\", \"eval_top1\"], \n",
    "  [f\"train_model_{i}.state.l1\" for i in (4, 6, 8)] + [\"train_model.output.l1\"],\n",
    "  #[f\"eval_model_{i}.weight.l1\" for i in (1, 3, 5, 7)],\n",
    "]\n",
    "for i, (db, on, mets) in enumerate(itertools.product(dbs, ons[:1], metsets)):\n",
    "  ncols = len(dos)*len(stds)\n",
    "  nrows = len(mets)\n",
    "  _, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, sharex=True, sharey=\"row\",\n",
    "    figsize=(ncols*2.5, nrows*2), dpi=100, facecolor=\"w\")\n",
    "  for j, (do, std) in enumerate(itertools.product(dos, stds)):\n",
    "    sdf = df[(df[\"db\"]==db)&(df[\"dim\"]==dim)&(df[\"do\"]==do)&(df[\"std\"]==std)&(df[\"on\"]==on)]\n",
    "    if len(sdf) == 0:\n",
    "      continue\n",
    "    sdf = sdf.drop(columns=[\"db\", \"dim\", \"do\", \"on\", \"std\"])\n",
    "    sdf = sdf.set_index(\"epoch\").groupby(\"lyr\")\n",
    "    for k, met in enumerate(mets):\n",
    "      sdf[met].plot(ax=axes[k][j], legend=False)\n",
    "      axes[k][j].set_xlabel(f\"{do} {on} {std}\")\n",
    "      axes[k][j].set_ylabel(met)\n",
    "      axes[k][j].grid()\n",
    "      if met == \"eval_top1\":\n",
    "        axes[k][j].set_ylim((0.45, 0.6))\n",
    "    axes[0][j].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(db, dim, do, on, std, lyr, layers, flip, axes):\n",
    "  ylim, xlim = [0, 2], [-.25, .25]\n",
    "  label = f\"{db}_{dim}_{do}_{on}_{std}_{lyr}_1\"\n",
    "  try:\n",
    "    state = torch.load(f\"outputs/{label}/analysis.pt\")\n",
    "  except:\n",
    "    return\n",
    "  for j, layer in enumerate(layers):\n",
    "    w = state[\"ml\"][\"w\"][layer] if flip else state[\"ml\"][\"w\"][layer+1].T\n",
    "    x = torch.matmul(w,  w.T)\n",
    "    y = state[\"ml\"][\"m2\"][layer]\n",
    "    axes[-j-1].hist2d(\n",
    "      x.flatten().numpy(), y.flatten().numpy(), \n",
    "      range=[xlim, ylim], bins=[50, 50],\n",
    "      norm=mpl.colors.LogNorm())\n",
    "  axes[-1].set_xlabel(f\"{do} {on} {std} {lyr}\")\n",
    "\n",
    "layers = range(3)\n",
    "for db, dim, on, lyr in itertools.product(dbs, dims, ons, lyrs[1:]):\n",
    "  nrows = len(layers)\n",
    "  ncols = len(dos)*len(stds)\n",
    "  _, axes = plt.subplots(\n",
    "    nrows=nrows, ncols=ncols, sharex=True, sharey=True,\n",
    "    figsize=(ncols*2, nrows*2), dpi=100, facecolor=\"w\")\n",
    "  for i, (do, std) in enumerate(itertools.product(dos, stds)):\n",
    "    sect = [row[i] for row in axes]\n",
    "    plot(db, dim, do, on, std, lyr, layers, False, sect)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
