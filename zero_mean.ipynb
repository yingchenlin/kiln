{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/cifar-10\"\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
    "trainset = torchvision.datasets.CIFAR10(path, train=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(path, train=False, transform=transform)\n",
    "\n",
    "def run(model, loss_fn, optim, num_epochs, batch_size):\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_top1 = 0\n",
    "        train_step = 0\n",
    "        progress = tqdm(trainloader, desc=f\"epoch={epoch} train\")\n",
    "        for inputs, targets in progress:\n",
    "            optim.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            top1 = (outputs.max(1).indices  == targets).float().mean()\n",
    "            train_loss += loss.item()\n",
    "            train_top1 += top1.item()\n",
    "            train_step += 1\n",
    "            progress.set_postfix({\n",
    "                \"train_loss\": train_loss / train_step,\n",
    "                \"train_top1\": train_top1 / train_step,\n",
    "            })\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_top1 = 0\n",
    "        test_step = 0\n",
    "        progress = tqdm(testloader, desc=f\"epoch={epoch} test\")\n",
    "        for inputs, targets in progress:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            top1 = (outputs.max(1).indices  == targets).float().mean()\n",
    "            test_loss += loss.item()\n",
    "            test_top1 += top1.item()\n",
    "            test_step += 1\n",
    "            progress.set_postfix({\n",
    "                \"test_loss\": test_loss / test_step,\n",
    "                \"test_top1\": test_top1 / test_step,\n",
    "            })\n",
    "\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / train_step,\n",
    "            \"train_top1\": train_top1 / train_step,\n",
    "            \"test_loss\": test_loss / test_step,\n",
    "            \"test_top1\": test_top1 / test_step,\n",
    "        }\n",
    "        yield metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=0 train: 100%|██████████| 500/500 [00:06<00:00, 74.08it/s, train_loss=1.87, train_top1=0.325]\n",
      "epoch=0 test: 100%|██████████| 100/100 [00:00<00:00, 107.62it/s, test_loss=1.65, test_top1=0.416]\n",
      "epoch=1 train: 100%|██████████| 500/500 [00:06<00:00, 79.30it/s, train_loss=1.66, train_top1=0.412]\n",
      "epoch=1 test: 100%|██████████| 100/100 [00:00<00:00, 116.79it/s, test_loss=1.53, test_top1=0.467]\n",
      "epoch=2 train: 100%|██████████| 500/500 [00:06<00:00, 77.17it/s, train_loss=1.58, train_top1=0.442]\n",
      "epoch=2 test: 100%|██████████| 100/100 [00:00<00:00, 115.23it/s, test_loss=1.49, test_top1=0.474]\n",
      "epoch=3 train: 100%|██████████| 500/500 [00:06<00:00, 78.36it/s, train_loss=1.53, train_top1=0.46] \n",
      "epoch=3 test: 100%|██████████| 100/100 [00:00<00:00, 111.01it/s, test_loss=1.43, test_top1=0.491]\n",
      "epoch=4 train: 100%|██████████| 500/500 [00:06<00:00, 75.82it/s, train_loss=1.49, train_top1=0.475]\n",
      "epoch=4 test: 100%|██████████| 100/100 [00:00<00:00, 112.38it/s, test_loss=1.41, test_top1=0.492]\n",
      "epoch=5 train: 100%|██████████| 500/500 [00:06<00:00, 75.86it/s, train_loss=1.45, train_top1=0.486]\n",
      "epoch=5 test: 100%|██████████| 100/100 [00:00<00:00, 108.37it/s, test_loss=1.39, test_top1=0.507]\n",
      "epoch=6 train: 100%|██████████| 500/500 [00:06<00:00, 75.21it/s, train_loss=1.43, train_top1=0.493]\n",
      "epoch=6 test: 100%|██████████| 100/100 [00:00<00:00, 105.16it/s, test_loss=1.39, test_top1=0.502]\n",
      "epoch=7 train: 100%|██████████| 500/500 [00:06<00:00, 72.31it/s, train_loss=1.41, train_top1=0.501]\n",
      "epoch=7 test: 100%|██████████| 100/100 [00:00<00:00, 105.64it/s, test_loss=1.38, test_top1=0.507]\n",
      "epoch=8 train: 100%|██████████| 500/500 [00:09<00:00, 54.50it/s, train_loss=1.38, train_top1=0.512]\n",
      "epoch=8 test: 100%|██████████| 100/100 [00:01<00:00, 82.03it/s, test_loss=1.35, test_top1=0.514]\n",
      "epoch=9 train: 100%|██████████| 500/500 [00:07<00:00, 67.71it/s, train_loss=1.37, train_top1=0.516]\n",
      "epoch=9 test: 100%|██████████| 100/100 [00:00<00:00, 104.04it/s, test_loss=1.35, test_top1=0.519]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 0,\n",
       "  'train_loss': 1.8701764814853667,\n",
       "  'train_top1': 0.3248000002503395,\n",
       "  'test_loss': 1.6479486954212188,\n",
       "  'test_top1': 0.4159999969601631},\n",
       " {'epoch': 1,\n",
       "  'train_loss': 1.6603232519626618,\n",
       "  'train_top1': 0.41151999807357786,\n",
       "  'test_loss': 1.5291156268119812,\n",
       "  'test_top1': 0.46729999750852586},\n",
       " {'epoch': 2,\n",
       "  'train_loss': 1.5824878711700439,\n",
       "  'train_top1': 0.44163999700546264,\n",
       "  'test_loss': 1.4897980260849,\n",
       "  'test_top1': 0.4738999956846237},\n",
       " {'epoch': 3,\n",
       "  'train_loss': 1.529931223630905,\n",
       "  'train_top1': 0.46035999757051466,\n",
       "  'test_loss': 1.4316531491279603,\n",
       "  'test_top1': 0.4905999964475632},\n",
       " {'epoch': 4,\n",
       "  'train_loss': 1.4912743566036224,\n",
       "  'train_top1': 0.4745799962878227,\n",
       "  'test_loss': 1.414997432231903,\n",
       "  'test_top1': 0.4920999965071678},\n",
       " {'epoch': 5,\n",
       "  'train_loss': 1.452074227333069,\n",
       "  'train_top1': 0.48647999674081804,\n",
       "  'test_loss': 1.394192453622818,\n",
       "  'test_top1': 0.507399995625019},\n",
       " {'epoch': 6,\n",
       "  'train_loss': 1.428001995563507,\n",
       "  'train_top1': 0.49309999638795854,\n",
       "  'test_loss': 1.389330840110779,\n",
       "  'test_top1': 0.5020999962091446},\n",
       " {'epoch': 7,\n",
       "  'train_loss': 1.408886554479599,\n",
       "  'train_top1': 0.5010999960899353,\n",
       "  'test_loss': 1.3804011166095733,\n",
       "  'test_top1': 0.5068999975919724},\n",
       " {'epoch': 8,\n",
       "  'train_loss': 1.3845212194919587,\n",
       "  'train_top1': 0.5120199965834618,\n",
       "  'test_loss': 1.3544860422611236,\n",
       "  'test_top1': 0.5144999971985817},\n",
       " {'epoch': 9,\n",
       "  'train_loss': 1.3662793707847596,\n",
       "  'train_top1': 0.5159399967193603,\n",
       "  'test_loss': 1.3531347954273223,\n",
       "  'test_top1': 0.5194999966025352}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dropout(nn.Module):\n",
    "\n",
    "    def __init__(self, std, zero_mean):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.zero_mean = zero_mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            z = torch.randn_like(x)\n",
    "            if self.zero_mean:\n",
    "                z = z - z.mean(-1, keepdim=True)\n",
    "            x = x * (1 + self.std * z)\n",
    "        return x\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "hidden_dim = 100\n",
    "learning_rate = 1e-2\n",
    "std = 0.5\n",
    "zero_mean = False\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3072, hidden_dim), nn.ReLU(), Dropout(std, zero_mean),\n",
    "    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), Dropout(std, zero_mean),\n",
    "    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), Dropout(std, zero_mean),\n",
    "    nn.Linear(hidden_dim, 10))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "list(run(model, loss_fn, optim, num_epochs, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ffbacaed3e7a1b7217631a96fb71275f46bb06db39755119e90aad0ec593d519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
